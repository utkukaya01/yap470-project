{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8922045c",
   "metadata": {},
   "source": [
    "# **Fake News Detection - Method 1 (Training)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15055146",
   "metadata": {},
   "source": [
    "Imported libraries and loaded functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c93682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# Download nltk assets\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"ner\"])\n",
    "\n",
    "# Prepare stop word set\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Precompile regex and punctuation map\n",
    "URL_PATTERN = re.compile(r\"http\\S+|www\\S+\")\n",
    "PUNCT_MAP = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Enable tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bedb6",
   "metadata": {},
   "source": [
    "Wrote a function to clean a given text\n",
    "- Lowercase\n",
    "- Strip URLs\n",
    "- Remove punctuation\n",
    "- Tokenize\n",
    "- Lemmatize\n",
    "- Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532100f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Lowercase, remove urls and punctuation\n",
    "    txt = text.lower()\n",
    "    txt = URL_PATTERN.sub(\" \", txt)\n",
    "    txt = txt.translate(PUNCT_MAP)\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    doc = nlp(txt)\n",
    "\n",
    "    # Filter out stop words and non alpha tokens\n",
    "    return \" \".join(tok.lemma_ for tok in doc if tok.is_alpha and tok.text not in STOP_WORDS)\n",
    "\n",
    "# An example output of the function\n",
    "example = \"Here's an Example: https://example.com Running, RUNNERS and ran!\"\n",
    "print(\"Input: \", example)\n",
    "print(\"Output: \", clean_text(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b767e01",
   "metadata": {},
   "source": [
    "Loaded, applied cleaning and splitted ISOT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_train_df = None\n",
    "\n",
    "# If processed data is present, load; else process\n",
    "if Path(\"data/processed/ISOT/isot_train.pkl\").exists() :\n",
    "    # Load train data\n",
    "    isot_train_df = joblib.load(\"data/processed/ISOT/isot_train.pkl\")\n",
    "\n",
    "    # Display count of rows\n",
    "    print(f\"ISOT processed count: {isot_train_df.shape[0]:,} rows\")\n",
    "\n",
    "    # Display first few rows\n",
    "    display(isot_train_df.head())\n",
    "else:\n",
    "    # Load dataset\n",
    "    isot_fake = pd.read_csv(\"data/raw/ISOT/Fake.csv\")\n",
    "    isot_true = pd.read_csv(\"data/raw/ISOT/True.csv\")\n",
    "\n",
    "    # Labels\n",
    "    isot_fake[\"label\"] = 0 # fake\n",
    "    isot_true[\"label\"] = 1 # true\n",
    "\n",
    "    # Concatenate true and fake datas by only keeping their text and label parts\n",
    "    isot_df = pd.concat([isot_fake[[\"text\",\"label\"]], isot_true[[\"text\",\"label\"]]], ignore_index=True) \\\n",
    "                .rename(columns={\"text\":\"content\"})\n",
    "\n",
    "    # Display count of rows\n",
    "    print(f\"ISOT raw count: {isot_df.shape[0]:,} rows\")\n",
    "\n",
    "    # Clean dataset\n",
    "    isot_df[\"cleaned\"] = isot_df[\"content\"].progress_apply(clean_text)\n",
    "\n",
    "    # Display first few rows\n",
    "    display(isot_df.head())\n",
    "\n",
    "    # Split cleaned dataset into train and test datas\n",
    "    isot_train_df, isot_test_df = train_test_split(isot_df, test_size=0.2, stratify=isot_df[\"label\"], random_state=42)\n",
    "\n",
    "    # Save datasets\n",
    "    isot_train_filename = Path(\"data/processed/ISOT\") / \"isot_train.pkl\"\n",
    "    isot_test_filename = Path(\"data/processed/ISOT\") / \"isot_test.pkl\"\n",
    "    joblib.dump(isot_train_df, isot_train_filename)\n",
    "    joblib.dump(isot_test_df, isot_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1f048",
   "metadata": {},
   "source": [
    "Loaded, applied cleaning and splitted LIAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125eedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_train_df = None\n",
    "\n",
    "# If processed data is present, load; else process\n",
    "if Path(\"data/processed/LIAR/liar_train.pkl\").exists():\n",
    "    # Load train data\n",
    "    liar_train_df = joblib.load(\"data/processed/LIAR/liar_train.pkl\")\n",
    "    \n",
    "    # Display count of rows\n",
    "    print(f\"LIAR processed count: {liar_train_df.shape[0]:,} rows\")\n",
    "\n",
    "    # Display first few rows\n",
    "    display(liar_train_df.head())\n",
    "else:\n",
    "    # Columns of dataset\n",
    "    liar_cols = [\n",
    "        \"id_json\",\"label\",\"statement\",\"subject\",\"speaker\",\"speaker_job\",\n",
    "        \"state_info\",\"party_affil\",\"barely_true\",\"false\",\"half_true\",\n",
    "        \"mostly_true\",\"pants_fire\",\"context\"\n",
    "    ]\n",
    "\n",
    "    # Load dataset\n",
    "    liar_train = pd.read_csv(\"data/raw/LIAR/train.tsv\", sep=\"\\t\", header=None, names=liar_cols)\n",
    "    liar_valid = pd.read_csv(\"data/raw/LIAR/valid.tsv\", sep=\"\\t\", header=None, names=liar_cols)\n",
    "    liar_test  = pd.read_csv(\"data/raw/LIAR/test.tsv\",  sep=\"\\t\", header=None, names=liar_cols)\n",
    "\n",
    "    # Concatenate train, valid and test datas\n",
    "    liar_df = pd.concat([liar_train, liar_valid, liar_test], ignore_index=True)\n",
    "\n",
    "    # Group labels to turn them into a binary classification\n",
    "    liar_df[\"label\"] = liar_df[\"label\"].map({\n",
    "        \"pants-fire\":0, \"false\":0, \"barely-true\":0, # fake\n",
    "        \"half-true\":1,  \"mostly-true\":1,  \"true\":1  # true\n",
    "    })\n",
    "    liar_df = liar_df[[\"statement\",\"label\"]].rename(columns={\"statement\":\"content\"})\n",
    "\n",
    "    # Display count of rows\n",
    "    print(f\"LIAR raw count: {liar_df.shape[0]:,} rows\")\n",
    "\n",
    "    # Clean dataset\n",
    "    liar_df[\"cleaned\"] = liar_df[\"content\"].progress_apply(clean_text)\n",
    "\n",
    "    # Display first few rows\n",
    "    display(liar_df.head())\n",
    "\n",
    "    # Split cleaned dataset into train and test datas\n",
    "    liar_train_df, liar_test_df = train_test_split(liar_df, test_size=0.2, stratify=liar_df[\"label\"], random_state=42)\n",
    "\n",
    "    # Save datasets\n",
    "    liar_train_filename = Path(\"data/processed/LIAR\") / \"liar_train.pkl\"\n",
    "    liar_test_filename = Path(\"data/processed/LIAR\") / \"liar_test.pkl\"\n",
    "    joblib.dump(liar_train_df, liar_train_filename)\n",
    "    joblib.dump(liar_test_df, liar_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3ee22",
   "metadata": {},
   "source": [
    "Extracted features using BoW and TF-IDF for ISOT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW pipeline\n",
    "bow_pipeline_isot = Pipeline([\n",
    "    (\"vect\", CountVectorizer(ngram_range=(1,3), min_df=10)), # word sequences and minimum appearance\n",
    "    (\"scale\", MaxAbsScaler())\n",
    "])\n",
    "\n",
    "# TF-IDF pipeline\n",
    "tfidf_pipeline_isot = Pipeline([\n",
    "    (\"vect\", TfidfVectorizer(ngram_range=(1,3), min_df=10)) # word sequences and minimum appearance\n",
    "])\n",
    "\n",
    "# Feature matrices for ISOT\n",
    "X_isot_bow = bow_pipeline_isot.fit_transform(isot_train_df[\"cleaned\"])\n",
    "X_isot_tfidf = tfidf_pipeline_isot.fit_transform(isot_train_df[\"cleaned\"])\n",
    "\n",
    "# Sample count and feature space size of BoW and TF-IDF\n",
    "print(\"ISOT BoW shape: \", X_isot_bow.shape)\n",
    "print(\"ISOT TF-IDF shape: \", X_isot_tfidf.shape)\n",
    "\n",
    "# Show first 10 feature names\n",
    "feature_names_bow = bow_pipeline_isot.named_steps[\"vect\"].get_feature_names_out()[:10]\n",
    "feature_names_tfidf = tfidf_pipeline_isot.named_steps[\"vect\"].get_feature_names_out()[:10]\n",
    "print(\"ISOT BoW features sample: \", feature_names_bow)\n",
    "print(\"ISOT TF-IDF features sample: \", feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec877f",
   "metadata": {},
   "source": [
    "Extracted features using BoW and TF-IDF for LIAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW pipeline\n",
    "bow_pipeline_liar = Pipeline([\n",
    "    (\"vect\", CountVectorizer(ngram_range=(1,3), min_df=5)), # word sequences and minimum appearance\n",
    "    (\"scale\", MaxAbsScaler())\n",
    "])\n",
    "\n",
    "# TF-IDF pipeline\n",
    "tfidf_pipeline_liar = Pipeline([\n",
    "    (\"vect\", TfidfVectorizer(ngram_range=(1,3), min_df=5))  # word sequences and minimum appearance\n",
    "])\n",
    "\n",
    "# Feature matrices for LIAR\n",
    "X_liar_bow = bow_pipeline_liar.fit_transform(liar_train_df[\"cleaned\"])\n",
    "X_liar_tfidf = tfidf_pipeline_liar.fit_transform(liar_train_df[\"cleaned\"])\n",
    "\n",
    "# Sample count and feature space size of BoW and TF-IDF\n",
    "print(\"LIAR BoW shape: \", X_liar_bow.shape)\n",
    "print(\"LIAR TF-IDF shape: \", X_liar_tfidf.shape)\n",
    "\n",
    "# Show first 10 feature names\n",
    "feature_names_bow = bow_pipeline_liar.named_steps[\"vect\"].get_feature_names_out()[:10]\n",
    "feature_names_tfidf = tfidf_pipeline_liar.named_steps[\"vect\"].get_feature_names_out()[:10]\n",
    "print(\"LIAR BoW features sample: \", feature_names_bow)\n",
    "print(\"LIAR TF-IDF features sample: \", feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd7e74",
   "metadata": {},
   "source": [
    "Created models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0dccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron\n",
    "mlp_parameters = {\n",
    "    \"hidden_layer_sizes\": [(128,), (128, 64)], # number of neurons per hidden layer\n",
    "    \"alpha\": np.logspace(-4, -2, 3) # L2 regularization strength\n",
    "}\n",
    "\n",
    "# Gradient Boosting Machine\n",
    "gbm_parameters = {\n",
    "    \"n_estimators\": [200, 400], # number of trees\n",
    "    \"learning_rate\": [0.05, 0.1], # learning rate\n",
    "    \"max_depth\": [3, 5] # maximum depth of trees\n",
    "}\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_parameters = {\n",
    "    \"C\": np.logspace(-1, 1, 3), # L2 regularization strength\n",
    "}\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"mlp\": (MLPClassifier(max_iter=100, random_state=42), mlp_parameters),\n",
    "    \"gbm\": (GradientBoostingClassifier(random_state=42), gbm_parameters),\n",
    "    \"svm\": (LinearSVC(random_state=42), svm_parameters)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fcbaf",
   "metadata": {},
   "source": [
    "Wrote a function to train and save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0367aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(X, y, dataset: str, feature_space: str):\n",
    "    # Stratified 5-fold cross validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Macro F1-score as evaluation metric\n",
    "    scorer = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "    for model_name, (base, grid) in models.items():\n",
    "        print(f\"Training on '{dataset}' with features of '{feature_space}' using '{model_name.upper()}'\")\n",
    "\n",
    "        # Randomized hyperparameter search with cross validation\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=base,\n",
    "            param_distributions=grid,\n",
    "            n_iter=4, # number of hyperparameter combinations to try\n",
    "            cv=cv,\n",
    "            scoring=scorer,\n",
    "            random_state=42,\n",
    "            n_jobs=8,\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Fit model and select best estimator\n",
    "        search.fit(X, y)\n",
    "        best = search.best_estimator_\n",
    "\n",
    "        # Save the model with best hyperparameters\n",
    "        filename = Path(\"saved_models/method1\") / f\"{dataset}_{feature_space}_{model_name.upper()}.pkl\"\n",
    "        joblib.dump(best, filename)\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b5aad",
   "metadata": {},
   "source": [
    "Trained and saved models for every dataset/feature pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04321e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_and_save(X_liar_bow, liar_train_df[\"label\"].values, \"LIAR\", \"BoW\")\n",
    "print(\"-----\")\n",
    "train_and_save(X_liar_tfidf, liar_train_df[\"label\"].values, \"LIAR\", \"TF-IDF\")\n",
    "print(\"-----\")\n",
    "train_and_save(X_isot_bow, isot_train_df[\"label\"].values, \"ISOT\", \"BoW\")\n",
    "print(\"-----\")\n",
    "train_and_save(X_isot_tfidf, isot_train_df[\"label\"].values, \"ISOT\", \"TF-IDF\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
