{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f230b7",
   "metadata": {},
   "source": [
    "# **Fake News Detection - Method 1 (Training)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f31c2",
   "metadata": {},
   "source": [
    "Load assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca857b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/uk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/uk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Download nltk assets\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\",\"ner\"])\n",
    "\n",
    "# Prepare stop word set\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Precompile regex and punctuation map\n",
    "URL_PATTERN = re.compile(r\"http\\S+|www\\S+\")\n",
    "PUNCT_MAP = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Enable tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6228b",
   "metadata": {},
   "source": [
    "A function to clean a given text\n",
    "- Lowercase\n",
    "- Strip URLs\n",
    "- Remove punctuation\n",
    "- Tokenize\n",
    "- Lemmatize\n",
    "- Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aeb0580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  Here's an Example: https://example.com Running, RUNNERS and ran!\n",
      "Output:  here example run runner run\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Lowercase, remove urls and punctuation\n",
    "    txt = text.lower()\n",
    "    txt = URL_PATTERN.sub(\" \", txt)\n",
    "    txt = txt.translate(PUNCT_MAP)\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    doc = nlp(txt)\n",
    "\n",
    "    # Filter out stop words and non alpha tokens\n",
    "    return \" \".join(tok.lemma_ for tok in doc if tok.is_alpha and tok.text not in STOP_WORDS)\n",
    "\n",
    "# An example output of the function\n",
    "example = \"Here's an Example: https://example.com Running, RUNNERS and ran!\"\n",
    "print(\"Input: \", example)\n",
    "print(\"Output: \", clean_text(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2da8f",
   "metadata": {},
   "source": [
    "Load, apply cleaning and split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a557bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# If processed data is not present, process\n",
    "if not Path(\"data/processed/ISOT/isot_train.pkl\").exists():\n",
    "    # Load dataset\n",
    "    isot_fake = pd.read_csv(\"data/raw/ISOT/Fake.csv\")\n",
    "    isot_true = pd.read_csv(\"data/raw/ISOT/True.csv\")\n",
    "\n",
    "    # Labels\n",
    "    isot_fake[\"label\"] = 0 # fake\n",
    "    isot_true[\"label\"] = 1 # true\n",
    "\n",
    "    # Concatenate true and fake datas by only keeping their text and label parts\n",
    "    isot_df = pd.concat([isot_fake[[\"text\",\"label\"]], isot_true[[\"text\",\"label\"]]], ignore_index=True) \\\n",
    "                .rename(columns={\"text\":\"content\"})\n",
    "\n",
    "    # Display count of rows\n",
    "    print(f\"ISOT raw count: {isot_df.shape[0]:,} rows\")\n",
    "\n",
    "    # Clean dataset\n",
    "    isot_df[\"cleaned\"] = isot_df[\"content\"].progress_apply(clean_text)\n",
    "\n",
    "    # Display first few rows\n",
    "    display(isot_df.head())\n",
    "\n",
    "    # Split cleaned dataset into train and test datas\n",
    "    isot_train_df, isot_test_df = train_test_split(isot_df, test_size=0.2, stratify=isot_df[\"label\"], random_state=42)\n",
    "\n",
    "    # Save datasets\n",
    "    isot_train_filename = Path(\"data/processed/ISOT\") / \"isot_train.pkl\"\n",
    "    isot_test_filename = Path(\"data/processed/ISOT\") / \"isot_test.pkl\"\n",
    "    joblib.dump(isot_train_df, isot_train_filename)\n",
    "    joblib.dump(isot_test_df, isot_test_filename)\n",
    "\n",
    "\n",
    "# If processed data is not present, process\n",
    "if not Path(\"data/processed/LIAR/liar_train.pkl\").exists():\n",
    "    # Columns of dataset\n",
    "    liar_cols = [\n",
    "        \"id_json\",\"label\",\"statement\",\"subject\",\"speaker\",\"speaker_job\",\n",
    "        \"state_info\",\"party_affil\",\"barely_true\",\"false\",\"half_true\",\n",
    "        \"mostly_true\",\"pants_fire\",\"context\"\n",
    "    ]\n",
    "\n",
    "    # Load dataset\n",
    "    liar_train = pd.read_csv(\"data/raw/LIAR/train.tsv\", sep=\"\\t\", header=None, names=liar_cols)\n",
    "    liar_valid = pd.read_csv(\"data/raw/LIAR/valid.tsv\", sep=\"\\t\", header=None, names=liar_cols)\n",
    "    liar_test  = pd.read_csv(\"data/raw/LIAR/test.tsv\",  sep=\"\\t\", header=None, names=liar_cols)\n",
    "\n",
    "    # Concatenate train, valid and test datas\n",
    "    liar_df = pd.concat([liar_train, liar_valid, liar_test], ignore_index=True)\n",
    "\n",
    "    # Group labels to turn them into a binary classification\n",
    "    liar_df[\"label\"] = liar_df[\"label\"].map({\n",
    "        \"pants-fire\":0, \"false\":0, \"barely-true\":0, # fake\n",
    "        \"half-true\":1,  \"mostly-true\":1,  \"true\":1  # true\n",
    "    })\n",
    "    liar_df = liar_df[[\"statement\",\"label\"]].rename(columns={\"statement\":\"content\"})\n",
    "\n",
    "    # Display count of rows\n",
    "    print(f\"LIAR raw count: {liar_df.shape[0]:,} rows\")\n",
    "\n",
    "    # Clean dataset\n",
    "    liar_df[\"cleaned\"] = liar_df[\"content\"].progress_apply(clean_text)\n",
    "\n",
    "    # Display first few rows\n",
    "    display(liar_df.head())\n",
    "\n",
    "    # Split cleaned dataset into train and test datas\n",
    "    liar_train_df, liar_test_df = train_test_split(liar_df, test_size=0.2, stratify=liar_df[\"label\"], random_state=42)\n",
    "\n",
    "    # Save datasets\n",
    "    liar_train_filename = Path(\"data/processed/LIAR\") / \"liar_train.pkl\"\n",
    "    liar_test_filename = Path(\"data/processed/LIAR\") / \"liar_test.pkl\"\n",
    "    joblib.dump(liar_train_df, liar_train_filename)\n",
    "    joblib.dump(liar_test_df, liar_test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c18522",
   "metadata": {},
   "source": [
    "Load train datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345c9791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISOT processed count: 35,918 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37645</th>\n",
       "      <td>PARIS (Reuters) - French bank Societe Generale...</td>\n",
       "      <td>1</td>\n",
       "      <td>paris reuters french bank societe generale wed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30390</th>\n",
       "      <td>WINSTON-SALEM, N.C. (Reuters) - North Carolina...</td>\n",
       "      <td>1</td>\n",
       "      <td>winstonsalem nc reuters north carolina governo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18191</th>\n",
       "      <td>Civil political discourse took a beating in We...</td>\n",
       "      <td>0</td>\n",
       "      <td>civil political discourse take beating west vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25384</th>\n",
       "      <td>(Reuters) - New York and Washington state on M...</td>\n",
       "      <td>1</td>\n",
       "      <td>reuters new york washington state monday vow s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32622</th>\n",
       "      <td>ORLANDO, Fla. (Reuters) - Orlando nightclub ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>orlando fla reuters orlando nightclub killer o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label  \\\n",
       "37645  PARIS (Reuters) - French bank Societe Generale...      1   \n",
       "30390  WINSTON-SALEM, N.C. (Reuters) - North Carolina...      1   \n",
       "18191  Civil political discourse took a beating in We...      0   \n",
       "25384  (Reuters) - New York and Washington state on M...      1   \n",
       "32622  ORLANDO, Fla. (Reuters) - Orlando nightclub ki...      1   \n",
       "\n",
       "                                                 cleaned  \n",
       "37645  paris reuters french bank societe generale wed...  \n",
       "30390  winstonsalem nc reuters north carolina governo...  \n",
       "18191  civil political discourse take beating west vi...  \n",
       "25384  reuters new york washington state monday vow s...  \n",
       "32622  orlando fla reuters orlando nightclub killer o...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIAR processed count: 10,232 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6920</th>\n",
       "      <td>On average, Americans spend less than 10 perce...</td>\n",
       "      <td>1</td>\n",
       "      <td>average americans spend less percent disposabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>The deficit this year could pay all of the 201...</td>\n",
       "      <td>1</td>\n",
       "      <td>deficit year could pay salary every profession...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9131</th>\n",
       "      <td>Farouk is on fire.</td>\n",
       "      <td>1</td>\n",
       "      <td>farouk fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>Says Jacky Rosen has refused to tell us whethe...</td>\n",
       "      <td>0</td>\n",
       "      <td>say jacky rosen refuse tell we whether support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12122</th>\n",
       "      <td>On Common Core.</td>\n",
       "      <td>0</td>\n",
       "      <td>common core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label  \\\n",
       "6920   On average, Americans spend less than 10 perce...      1   \n",
       "3934   The deficit this year could pay all of the 201...      1   \n",
       "9131                                  Farouk is on fire.      1   \n",
       "2685   Says Jacky Rosen has refused to tell us whethe...      0   \n",
       "12122                                    On Common Core.      0   \n",
       "\n",
       "                                                 cleaned  \n",
       "6920   average americans spend less percent disposabl...  \n",
       "3934   deficit year could pay salary every profession...  \n",
       "9131                                         farouk fire  \n",
       "2685   say jacky rosen refuse tell we whether support...  \n",
       "12122                                        common core  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load train data\n",
    "isot_train_df = joblib.load(\"data/processed/ISOT/isot_train.pkl\")\n",
    "\n",
    "# Display count of rows\n",
    "print(f\"ISOT processed count: {isot_train_df.shape[0]:,} rows\")\n",
    "\n",
    "# Display first few rows\n",
    "display(isot_train_df.head())\n",
    "\n",
    "\n",
    "# Load train data\n",
    "liar_train_df = joblib.load(\"data/processed/LIAR/liar_train.pkl\")\n",
    "\n",
    "# Display count of rows\n",
    "print(f\"LIAR processed count: {liar_train_df.shape[0]:,} rows\")\n",
    "\n",
    "# Display first few rows\n",
    "display(liar_train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60167524",
   "metadata": {},
   "source": [
    "Create a dictionary out of cleaned train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6474c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"ISOT\": isot_train_df,\n",
    "    \"LIAR\": liar_train_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1eb7e1",
   "metadata": {},
   "source": [
    "Feature extraction constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179f7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECT_TYPES = [\"bow\", \"tfidf\"] # vectorization type\n",
    "MIN_DFS = [30, 40] # minimum appearance value\n",
    "REDUCTIONS = [\"svd\", \"chi2\"] # reduction type\n",
    "SVD_COMPONENTS = [300, 500] # maximum feature size\n",
    "CHI2_K_VALS    = [500, 700] # maximum feature size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09c953",
   "metadata": {},
   "source": [
    "Functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f3970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "\n",
    "# Builds vectorizer according to given type and minimum appearance\n",
    "def build_vectorizer(vtype: str, min_df: int):\n",
    "    if vtype == \"bow\":\n",
    "        return CountVectorizer(ngram_range=(1, 3), lowercase=True, min_df=min_df)\n",
    "    elif vtype == \"tfidf\":\n",
    "        return TfidfVectorizer(ngram_range=(1, 3), lowercase=True, min_df=min_df, norm=\"l2\")\n",
    "    else:\n",
    "        raise ValueError(\"vtype must be 'bow' or 'tfidf'\")\n",
    "\n",
    "# Builds filepaths for features\n",
    "def feature_path(dataset: str, vtype: str, min_df: int, red: str, param: int) -> Path:\n",
    "    return Path(\"data/features\") / dataset / f\"{vtype}_min{min_df}_{red}{param}.joblib\"\n",
    "\n",
    "# Applies several different feature extraction methods and saves the generated features\n",
    "def generate_features(dataset: str, texts, labels):\n",
    "    for vtype in VECT_TYPES:\n",
    "        for min_df in MIN_DFS:\n",
    "            vect = build_vectorizer(vtype, min_df)\n",
    "            X_vect = vect.fit_transform(texts)\n",
    "\n",
    "            # TruncatedSVD variants\n",
    "            for n_comp in SVD_COMPONENTS:\n",
    "                f_path = feature_path(dataset, vtype, min_df, \"svd\", n_comp)\n",
    "                print(f\"\\nGenerating SVD features: dataset={dataset}, vtype={vtype}, min_df={min_df}, n_comp={n_comp}\")\n",
    "                if f_path.exists():\n",
    "                    print(f\"Output shape: {joblib.load(f_path)[\"X\"].shape}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    n = min(n_comp, X_vect.shape[1] - 1)\n",
    "                    svd = TruncatedSVD(n_components=n, algorithm=\"randomized\", random_state=42)\n",
    "                    X_red = svd.fit_transform(X_vect)\n",
    "                    print(f\"Output shape: {X_red.shape}\")\n",
    "                    joblib.dump({\"vect\": vect, \"svd\": svd, \"X\": X_red, \"y\": labels}, f_path)\n",
    "\n",
    "            # Chi-squared variants\n",
    "            for k_val in CHI2_K_VALS:\n",
    "                f_path = feature_path(dataset, vtype, min_df, \"chi2\", k_val)\n",
    "                print(f\"\\nGenerating Chi2 features: dataset={dataset}, vtype={vtype}, min_df={min_df}, k_val={k_val}\")\n",
    "                if f_path.exists():\n",
    "                    print(f\"Output shape: {joblib.load(f_path)[\"X\"].shape}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    k = min(k_val, X_vect.shape[1] - 1)\n",
    "                    chi = SelectKBest(chi2, k=k)\n",
    "                    X_red = chi.fit_transform(X_vect, labels)\n",
    "                    print(f\"Output shape: {X_red.shape}\")\n",
    "                    joblib.dump({\"vect\": vect, \"chi2\": chi, \"X\": X_red, \"y\": labels}, f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26abfba",
   "metadata": {},
   "source": [
    "Extract features from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98802db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=bow, min_df=30, n_comp=300\n",
      "Output shape: (35918, 300)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=bow, min_df=30, n_comp=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=bow, min_df=30, k_val=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=bow, min_df=30, k_val=700\n",
      "Output shape: (35918, 700)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=bow, min_df=40, n_comp=300\n",
      "Output shape: (35918, 300)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=bow, min_df=40, n_comp=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=bow, min_df=40, k_val=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=bow, min_df=40, k_val=700\n",
      "Output shape: (35918, 700)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=tfidf, min_df=30, n_comp=300\n",
      "Output shape: (35918, 300)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=tfidf, min_df=30, n_comp=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=tfidf, min_df=30, k_val=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=tfidf, min_df=30, k_val=700\n",
      "Output shape: (35918, 700)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=tfidf, min_df=40, n_comp=300\n",
      "Output shape: (35918, 300)\n",
      "\n",
      "Generating SVD features: dataset=ISOT, vtype=tfidf, min_df=40, n_comp=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=tfidf, min_df=40, k_val=500\n",
      "Output shape: (35918, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=ISOT, vtype=tfidf, min_df=40, k_val=700\n",
      "Output shape: (35918, 700)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=bow, min_df=30, n_comp=300\n",
      "Output shape: (10232, 300)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=bow, min_df=30, n_comp=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=bow, min_df=30, k_val=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=bow, min_df=30, k_val=700\n",
      "Output shape: (10232, 700)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=bow, min_df=40, n_comp=300\n",
      "Output shape: (10232, 300)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=bow, min_df=40, n_comp=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=bow, min_df=40, k_val=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=bow, min_df=40, k_val=700\n",
      "Output shape: (10232, 561)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=tfidf, min_df=30, n_comp=300\n",
      "Output shape: (10232, 300)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=tfidf, min_df=30, n_comp=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=tfidf, min_df=30, k_val=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=tfidf, min_df=30, k_val=700\n",
      "Output shape: (10232, 700)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=tfidf, min_df=40, n_comp=300\n",
      "Output shape: (10232, 300)\n",
      "\n",
      "Generating SVD features: dataset=LIAR, vtype=tfidf, min_df=40, n_comp=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=tfidf, min_df=40, k_val=500\n",
      "Output shape: (10232, 500)\n",
      "\n",
      "Generating Chi2 features: dataset=LIAR, vtype=tfidf, min_df=40, k_val=700\n",
      "Output shape: (10232, 561)\n"
     ]
    }
   ],
   "source": [
    "for ds, df in datasets.items():\n",
    "    generate_features(dataset=ds, texts=df[\"cleaned\"].values, labels=df[\"label\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a11851",
   "metadata": {},
   "source": [
    "Models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddcfd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "models = {\n",
    "    # Gradient Boosting Machine\n",
    "    \"gbm\": {\n",
    "        \"estimator\": HistGradientBoostingClassifier(random_state=42),\n",
    "        \"param_dist\": {\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"max_depth\": [2, 3]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Support Vector Machine\n",
    "    \"svm\": {\n",
    "        \"estimator\": LinearSVC(class_weight=\"balanced\", random_state=42),\n",
    "        \"param_dist\": {\n",
    "            \"C\": np.logspace(-2, 1, 5)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86367d",
   "metadata": {},
   "source": [
    "A function to train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b8531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "\n",
    "# Train a model with RandomizedSearchCV\n",
    "def train_model(X, y, ftr_tag: str, model_key: str):\n",
    "    mfile = Path(\"saved_models/method1\") / f\"{ftr_tag}_{model_key}.joblib\"\n",
    "    if mfile.exists():\n",
    "        return\n",
    "\n",
    "    model = models[model_key]\n",
    "    cv  = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # 5 folds\n",
    "\n",
    "    # Make matrix dense if it is sparse for GBM\n",
    "    if model_key in {\"gbm\"} and sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model[\"estimator\"],\n",
    "        param_distributions=model[\"param_dist\"],\n",
    "        n_iter=2,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=4,\n",
    "        cv=cv,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    ).fit(X, y)\n",
    "\n",
    "    joblib.dump(search.best_estimator_, mfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c353e9",
   "metadata": {},
   "source": [
    "Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "105298bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=30, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=bow, min_df=40, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=30, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=ISOT, vtype=tfidf, min_df=40, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=30, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=bow, min_df=40, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=30, reduction=chi2, k_val=700, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=svd, n_comp=300, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=svd, n_comp=300, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=svd, n_comp=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=svd, n_comp=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=chi2, k_val=500, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=chi2, k_val=500, model=svm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=chi2, k_val=700, model=gbm\n",
      "\n",
      "Training on: dataset=LIAR, vtype=tfidf, min_df=40, reduction=chi2, k_val=700, model=svm\n"
     ]
    }
   ],
   "source": [
    "for ds in datasets.keys():\n",
    "    for vtype in VECT_TYPES:\n",
    "        for min_df in MIN_DFS:\n",
    "\n",
    "            # SVD combinations\n",
    "            for n in SVD_COMPONENTS:\n",
    "                ftr_obj = joblib.load(feature_path(ds, vtype, min_df, \"svd\", n))\n",
    "                X, y = ftr_obj[\"X\"], ftr_obj[\"y\"]\n",
    "                ftr_tag  = f\"{ds.lower()}_{vtype}_min{min_df}_svd{n}\"\n",
    "                for model in models.keys():\n",
    "                    print(f\"\\nTraining on: dataset={ds}, vtype={vtype}, min_df={min_df}, reduction=svd, n_comp={n}, model={model}\")\n",
    "                    train_model(X, y, ftr_tag, model)\n",
    "\n",
    "            # Chi-squared combinations\n",
    "            for k in CHI2_K_VALS:\n",
    "                ftr_obj = joblib.load(feature_path(ds, vtype, min_df, \"chi2\", k))\n",
    "                X, y = ftr_obj[\"X\"], ftr_obj[\"y\"]\n",
    "                ftr_tag  = f\"{ds.lower()}_{vtype}_min{min_df}_chi2{k}\"\n",
    "                for model in models.keys():\n",
    "                    print(f\"\\nTraining on: dataset={ds}, vtype={vtype}, min_df={min_df}, reduction=chi2, k_val={k}, model={model}\")\n",
    "                    train_model(X, y, ftr_tag, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
